{"episode_reward": 0.0, "episode": 1.0, "duration": 2.7428836822509766, "step": 250}
{"episode_reward": 14.890925886808766, "episode": 2.0, "duration": 0.4158027172088623, "step": 500}
{"episode_reward": 12.01709556043722, "episode": 3.0, "duration": 0.42368245124816895, "step": 750}
{"episode_reward": 11.689882113178367, "episode": 4.0, "duration": 0.4446380138397217, "step": 1000}
{"episode_reward": 8.452714962303755, "episode": 5.0, "critic_loss": 0.03608058229628637, "actor_loss": -3.0127183054590407, "actor_mean_entropy": 7.743307247492766, "alpha_loss": 0.936694939842852, "alpha_value": 0.09465249271491692, "duration": 139.72533559799194, "step": 1250}
{"episode_reward": 12.39794432477241, "episode": 6.0, "critic_loss": 0.04628915824741125, "actor_loss": -5.102982769012451, "actor_mean_entropy": 7.999886329650879, "alpha_loss": 0.8807893519401551, "alpha_value": 0.08908601328875262, "duration": 24.34551191329956, "step": 1500}
{"episode_reward": 24.728032525660936, "episode": 7.0, "critic_loss": 0.051649208940565586, "actor_loss": -5.412984134674073, "actor_mean_entropy": 8.035086471557618, "alpha_loss": 0.8662616515159607, "alpha_value": 0.08803592133127469, "duration": 24.716366052627563, "step": 1750}
{"episode_reward": 14.400901548313348, "episode": 8.0, "critic_loss": 0.039882641166448594, "actor_loss": -5.785804321289063, "actor_mean_entropy": 8.082910980224609, "alpha_loss": 0.8512151937484741, "alpha_value": 0.08700566668115009, "duration": 24.56444787979126, "step": 2000}
{"episode_reward": 19.610758815871893, "episode": 9.0, "critic_loss": 0.04991712228953838, "actor_loss": -6.173799694061279, "actor_mean_entropy": 8.044802726745605, "alpha_loss": 0.8331647925376892, "alpha_value": 0.08599717000005354, "duration": 24.385639667510986, "step": 2250}
{"episode_reward": 27.360175564610522, "episode": 10.0, "critic_loss": 0.062368141651153564, "actor_loss": -6.522386177062988, "actor_mean_entropy": 8.044444622039794, "alpha_loss": 0.8161325325965881, "alpha_value": 0.08501055513019214, "duration": 24.38089895248413, "step": 2500}
{"episode_reward": 20.92267619665999, "episode": 11.0, "critic_loss": 0.07301510179042817, "actor_loss": -6.908747993469238, "actor_mean_entropy": 8.00410708618164, "alpha_loss": 0.800217945098877, "alpha_value": 0.08404361912126347, "duration": 24.520572185516357, "step": 2750}
{"episode_reward": 36.84155209229641, "episode": 12.0, "critic_loss": 0.08970619487762452, "actor_loss": -7.2630993804931645, "actor_mean_entropy": 7.968903827667236, "alpha_loss": 0.7839600009918213, "alpha_value": 0.083092998399799, "duration": 24.617477655410767, "step": 3000}
{"episode_reward": 38.02956210863337, "episode": 13.0, "critic_loss": 0.0704395446330309, "actor_loss": -7.684230869293213, "actor_mean_entropy": 7.946024345397949, "alpha_loss": 0.7622416410446167, "alpha_value": 0.08216481872818215, "duration": 24.531831741333008, "step": 3250}
{"episode_reward": 29.072137588841578, "episode": 14.0, "critic_loss": 0.09196024331450463, "actor_loss": -8.033193954467773, "actor_mean_entropy": 7.907505519866944, "alpha_loss": 0.7518906388282776, "alpha_value": 0.08125363375789796, "duration": 24.58308506011963, "step": 3500}
{"episode_reward": 40.26620237415133, "episode": 15.0, "critic_loss": 0.09743847969174385, "actor_loss": -8.42095530319214, "actor_mean_entropy": 7.89533576965332, "alpha_loss": 0.7363492980003357, "alpha_value": 0.08035525970423017, "duration": 24.713342428207397, "step": 3750}
{"episode_reward": 49.36122119464562, "episode": 16.0, "critic_loss": 0.10336015465855598, "actor_loss": -8.777619171142579, "actor_mean_entropy": 7.937311233520508, "alpha_loss": 0.7216426124572753, "alpha_value": 0.07946902185616754, "duration": 24.625308990478516, "step": 4000}
{"episode_reward": 52.1236652753454, "episode": 17.0, "critic_loss": 0.1047226131260395, "actor_loss": -9.189820724487305, "actor_mean_entropy": 7.84701876449585, "alpha_loss": 0.6971424069404603, "alpha_value": 0.07860379302490052, "duration": 24.498283624649048, "step": 4250}
{"episode_reward": 40.37035426911443, "episode": 18.0, "critic_loss": 0.13744821864366533, "actor_loss": -9.580720108032226, "actor_mean_entropy": 7.856684158325195, "alpha_loss": 0.687814386844635, "alpha_value": 0.07775703649485226, "duration": 24.50846266746521, "step": 4500}
{"episode_reward": 53.70001567525802, "episode": 19.0, "critic_loss": 0.13400045269727706, "actor_loss": -10.011193389892577, "actor_mean_entropy": 7.856680778503418, "alpha_loss": 0.681551833152771, "alpha_value": 0.07691210332497148, "duration": 24.527573347091675, "step": 4750}
{"episode_reward": 56.04578538670389, "episode": 20.0, "critic_loss": 0.1537071438729763, "actor_loss": -10.447935531616212, "actor_mean_entropy": 7.80358362197876, "alpha_loss": 0.6557604784965515, "alpha_value": 0.07608357052087772, "duration": 24.505091667175293, "step": 5000}
{"episode_reward": 76.86687435883888, "episode": 21.0, "critic_loss": 0.15400823220610618, "actor_loss": -10.850519477844239, "actor_mean_entropy": 7.742023567199707, "alpha_loss": 0.6325579681396485, "alpha_value": 0.07528396086974243, "duration": 24.527549266815186, "step": 5250}
{"episode_reward": 47.87157073656426, "episode": 22.0, "critic_loss": 0.15783489486575128, "actor_loss": -11.223016944885254, "actor_mean_entropy": 7.795514442443848, "alpha_loss": 0.6289935240745544, "alpha_value": 0.07448892245258897, "duration": 24.461683988571167, "step": 5500}
{"episode_reward": 51.57715142636973, "episode": 23.0, "critic_loss": 0.1841140374839306, "actor_loss": -11.62045280456543, "actor_mean_entropy": 7.752572933197022, "alpha_loss": 0.6167029228210449, "alpha_value": 0.07370164141049766, "duration": 24.56982445716858, "step": 5750}
{"episode_reward": 73.4799507093132, "episode": 24.0, "critic_loss": 0.19122334536910057, "actor_loss": -11.99055495452881, "actor_mean_entropy": 7.76466781616211, "alpha_loss": 0.6023357892036438, "alpha_value": 0.07292366020169797, "duration": 24.67413830757141, "step": 6000}
{"episode_reward": 62.10308762269177, "episode": 25.0, "critic_loss": 0.20058504259586335, "actor_loss": -12.465865966796875, "actor_mean_entropy": 7.714590839385986, "alpha_loss": 0.586752670288086, "alpha_value": 0.07216150238407813, "duration": 24.65279197692871, "step": 6250}
{"episode_reward": 99.30488098048183, "episode": 26.0, "critic_loss": 0.22655715394020082, "actor_loss": -12.883640579223632, "actor_mean_entropy": 7.662652999877929, "alpha_loss": 0.5716000752449035, "alpha_value": 0.07141032842841293, "duration": 24.381434440612793, "step": 6500}
{"episode_reward": 73.11654817418653, "episode": 27.0, "critic_loss": 0.2037431102991104, "actor_loss": -13.288431701660157, "actor_mean_entropy": 7.67257991027832, "alpha_loss": 0.5621584763526917, "alpha_value": 0.07066886038847865, "duration": 24.470936059951782, "step": 6750}
{"episode_reward": 63.639007606175646, "episode": 28.0, "critic_loss": 0.206900337100029, "actor_loss": -13.722169105529785, "actor_mean_entropy": 7.658813735961914, "alpha_loss": 0.5486954908370971, "alpha_value": 0.06993534538635368, "duration": 24.460031747817993, "step": 7000}
{"episode_reward": 76.70138989325025, "episode": 29.0, "critic_loss": 0.22869367665052415, "actor_loss": -14.143619270324708, "actor_mean_entropy": 7.646695095062256, "alpha_loss": 0.5362896552085876, "alpha_value": 0.0692129529764033, "duration": 24.518170833587646, "step": 7250}
{"episode_reward": 99.57793358558872, "episode": 30.0, "critic_loss": 0.215596724152565, "actor_loss": -14.553337783813477, "actor_mean_entropy": 7.607484397888183, "alpha_loss": 0.5271435406208038, "alpha_value": 0.06849873094483958, "duration": 24.689788103103638, "step": 7500}
{"episode_reward": 82.8912347589558, "episode": 31.0, "critic_loss": 0.231856468975544, "actor_loss": -15.004096954345703, "actor_mean_entropy": 7.6200707817077635, "alpha_loss": 0.5147205431461335, "alpha_value": 0.06778987096623772, "duration": 24.81825566291809, "step": 7750}
{"episode_reward": 50.1647397814634, "episode": 32.0, "critic_loss": 0.22554868745803833, "actor_loss": -15.392675468444825, "actor_mean_entropy": 7.584335414886475, "alpha_loss": 0.5073607385158538, "alpha_value": 0.06709148780598205, "duration": 24.52353286743164, "step": 8000}
{"episode_reward": 89.14038173482885, "episode": 33.0, "critic_loss": 0.24263349056243896, "actor_loss": -15.826087287902832, "actor_mean_entropy": 7.592361846923828, "alpha_loss": 0.49775949501991273, "alpha_value": 0.06639766896761232, "duration": 24.623067617416382, "step": 8250}
{"episode_reward": 104.98372453124954, "episode": 34.0, "critic_loss": 0.24436634361743928, "actor_loss": -16.21514454650879, "actor_mean_entropy": 7.573374324798584, "alpha_loss": 0.4901190264225006, "alpha_value": 0.06570718424889692, "duration": 24.68929648399353, "step": 8500}
{"episode_reward": 62.88204159940999, "episode": 35.0, "critic_loss": 0.29157259660959245, "actor_loss": -16.611525550842284, "actor_mean_entropy": 7.586989364624023, "alpha_loss": 0.4752962293624878, "alpha_value": 0.06502940598849055, "duration": 24.571528673171997, "step": 8750}
{"episode_reward": 78.44935906912973, "episode": 36.0, "critic_loss": 0.25508308243751526, "actor_loss": -17.00462890625, "actor_mean_entropy": 7.459675598144531, "alpha_loss": 0.4566660296916962, "alpha_value": 0.06436646570278874, "duration": 24.66306972503662, "step": 9000}
{"episode_reward": 75.92292150401443, "episode": 37.0, "critic_loss": 0.30457818949222565, "actor_loss": -17.384100204467774, "actor_mean_entropy": 7.492098003387452, "alpha_loss": 0.44774201393127444, "alpha_value": 0.06371712402418077, "duration": 21.765949249267578, "step": 9250}
{"episode_reward": 93.9633262863061, "episode": 38.0, "critic_loss": 0.36065304481983185, "actor_loss": -17.795410751342775, "actor_mean_entropy": 7.468231590270996, "alpha_loss": 0.4359169366359711, "alpha_value": 0.06307635957919631, "duration": 16.403987169265747, "step": 9500}
{"episode_reward": 117.78814234731023, "episode": 39.0, "critic_loss": 0.3842119621038437, "actor_loss": -18.240338973999023, "actor_mean_entropy": 7.407898349761963, "alpha_loss": 0.429675815820694, "alpha_value": 0.062439265616550284, "duration": 16.411780834197998, "step": 9750}
{"episode_reward": 85.18515702490366, "episode": 40.0, "critic_loss": 0.3618401764035225, "actor_loss": -18.6057169342041, "actor_mean_entropy": 7.3603452186584475, "alpha_loss": 0.41197555232048033, "alpha_value": 0.06181332911272794, "duration": 16.343816995620728, "step": 10000}
